<!DOCTYPE html>
<!-- START: inst/pkgdown/templates/layout.html --><!-- Generated by pkgdown: do not edit by hand --><html lang="en" data-bs-theme="auto">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<title>Sensing and tracking 3D environments: All in One View</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<script src="../assets/themetoggle.js"></script><link rel="stylesheet" type="text/css" href="../assets/styles.css">
<script src="../assets/scripts.js" type="text/javascript"></script><!-- mathjax --><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      config: ["MMLorHTML.js"],
      jax: ["input/TeX","input/MathML","output/HTML-CSS","output/NativeMML", "output/PreviewHTML"],
      extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
      TeX: {
        extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
      },
      tex2jax: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [ ['$$','$$'], ['\\[', '\\]'] ],
        processEscapes: true
      }
    });
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><!-- Responsive Favicon for The Carpentries --><link rel="apple-touch-icon" sizes="180x180" href="../favicons/incubator/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicons/incubator/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="../favicons/incubator/favicon-16x16.png">
<link rel="manifest" href="../favicons/incubator/site.webmanifest">
<link rel="mask-icon" href="../favicons/incubator/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" media="(prefers-color-scheme: light)" content="white">
<meta name="theme-color" media="(prefers-color-scheme: dark)" content="black">
</head>
<body>
    <header id="top" class="navbar navbar-expand-md top-nav incubator"><svg xmlns="http://www.w3.org/2000/svg" class="d-none"><symbol id="check2" viewbox="0 0 16 16"><path d="M13.854 3.646a.5.5 0 0 1 0 .708l-7 7a.5.5 0 0 1-.708 0l-3.5-3.5a.5.5 0 1 1 .708-.708L6.5 10.293l6.646-6.647a.5.5 0 0 1 .708 0z"></path></symbol><symbol id="circle-half" viewbox="0 0 16 16"><path d="M8 15A7 7 0 1 0 8 1v14zm0 1A8 8 0 1 1 8 0a8 8 0 0 1 0 16z"></path></symbol><symbol id="moon-stars-fill" viewbox="0 0 16 16"><path d="M6 .278a.768.768 0 0 1 .08.858 7.208 7.208 0 0 0-.878 3.46c0 4.021 3.278 7.277 7.318 7.277.527 0 1.04-.055 1.533-.16a.787.787 0 0 1 .81.316.733.733 0 0 1-.031.893A8.349 8.349 0 0 1 8.344 16C3.734 16 0 12.286 0 7.71 0 4.266 2.114 1.312 5.124.06A.752.752 0 0 1 6 .278z"></path><path d="M10.794 3.148a.217.217 0 0 1 .412 0l.387 1.162c.173.518.579.924 1.097 1.097l1.162.387a.217.217 0 0 1 0 .412l-1.162.387a1.734 1.734 0 0 0-1.097 1.097l-.387 1.162a.217.217 0 0 1-.412 0l-.387-1.162A1.734 1.734 0 0 0 9.31 6.593l-1.162-.387a.217.217 0 0 1 0-.412l1.162-.387a1.734 1.734 0 0 0 1.097-1.097l.387-1.162zM13.863.099a.145.145 0 0 1 .274 0l.258.774c.115.346.386.617.732.732l.774.258a.145.145 0 0 1 0 .274l-.774.258a1.156 1.156 0 0 0-.732.732l-.258.774a.145.145 0 0 1-.274 0l-.258-.774a1.156 1.156 0 0 0-.732-.732l-.774-.258a.145.145 0 0 1 0-.274l.774-.258c.346-.115.617-.386.732-.732L13.863.1z"></path></symbol><symbol id="sun-fill" viewbox="0 0 16 16"><path d="M8 12a4 4 0 1 0 0-8 4 4 0 0 0 0 8zM8 0a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 0zm0 13a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 13zm8-5a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2a.5.5 0 0 1 .5.5zM3 8a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2A.5.5 0 0 1 3 8zm10.657-5.657a.5.5 0 0 1 0 .707l-1.414 1.415a.5.5 0 1 1-.707-.708l1.414-1.414a.5.5 0 0 1 .707 0zm-9.193 9.193a.5.5 0 0 1 0 .707L3.05 13.657a.5.5 0 0 1-.707-.707l1.414-1.414a.5.5 0 0 1 .707 0zm9.193 2.121a.5.5 0 0 1-.707 0l-1.414-1.414a.5.5 0 0 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .707zM4.464 4.465a.5.5 0 0 1-.707 0L2.343 3.05a.5.5 0 1 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .708z"></path></symbol></svg><a class="visually-hidden-focusable skip-link" href="#main-content">Skip to main content</a>
  <div class="container-fluid top-nav-container">
    <div class="col-md-8">
      <div class="large-logo">
        <img id="incubator-logo" alt="Carpentries Incubator" src="../assets/images/incubator-logo.svg">
</div>
    </div>
    <div class="selector-container">
      <div id="theme-selector">
        <li class="nav-item dropdown" id="theme-button-list">
          <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
            <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="bd-theme-text">
<li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                Light
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                Dark
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                Auto
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
          </ul>
</li>
      </div>

      <div class="dropdown" id="instructor-dropdown">
        <button class="btn btn-secondary dropdown-toggle bordered-button" type="button" id="dropdownMenu1" data-bs-toggle="dropdown" aria-expanded="false">
          <i aria-hidden="true" class="icon" data-feather="eye"></i> Instructor View <i data-feather="chevron-down"></i>
        </button>
        <ul class="dropdown-menu" aria-labelledby="dropdownMenu1">
<li><button class="dropdown-item" type="button" onclick="window.location.href='../aio.html';">Learner View</button></li>
        </ul>
</div>
    </div>
  </div>
  <hr></header><nav class="navbar navbar-expand-xl bottom-nav incubator" aria-label="Main Navigation"><div class="container-fluid nav-container">
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle Navigation">
      <span class="navbar-toggler-icon"></span>
      <span class="menu-title">Menu</span>
    </button>
    <div class="nav-logo">
      <img class="small-logo" alt="Carpentries Incubator" src="../assets/images/incubator-logo-sm.svg">
</div>
    <div class="lesson-title-md">
      Sensing and tracking 3D environments
    </div>
    <div class="search-icon-sm">
      <!-- TODO: do not show until we have search
        <i role="img" aria-label="Search the All In One page" data-feather="search"></i>
      -->
    </div>
    <div class="desktop-nav">
      <ul class="navbar-nav me-auto mb-2 mb-lg-0">
<li class="nav-item">
          <span class="lesson-title">
            Sensing and tracking 3D environments
          </span>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/key-points.html">Key Points</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/instructor-notes.html">Instructor Notes</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/images.html">Extract All Images</a>
        </li>
        <li class="nav-item dropdown">
          <button class="nav-link dropdown-toggle" id="navbarDropdown" data-bs-toggle="dropdown" aria-expanded="false">
            More <i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu" aria-labelledby="navbarDropdown">
<hr>
<li><a class="dropdown-item" href="reference.html">Reference</a></li>
          </ul>
</li>
      </ul>
</div>
    <!--
    <form class="d-flex col-md-2 search-form">
      <fieldset disabled>
      <input class="form-control me-2 searchbox" type="search" placeholder="" aria-label="">
        <button class="btn btn-outline-success tablet-search-button"  type="submit">
          <i class="search-icon" data-feather="search" role="img" aria-label="Search the All In One page"></i>
        </button>
      </fieldset>
    </form>
    -->
    <a id="search-button" class="btn btn-primary" href="../instructor/aio.html" role="button" aria-label="Search the All In One page">Search the All In One page</a>
  </div>
<!--/div.container-fluid -->
</nav><div class="col-md-12 mobile-title">
  Sensing and tracking 3D environments
</div>

<aside class="col-md-12 lesson-progress"><div style="width: %" class="percentage">
    %
  </div>
  <div class="progress incubator">
    <div class="progress-bar incubator" role="progressbar" style="width: %" aria-valuenow="" aria-label="Lesson Progress" aria-valuemin="0" aria-valuemax="100">
    </div>
  </div>
</aside><div class="container">
      <div class="row">
        <!-- START: inst/pkgdown/templates/navbar.html -->
<div id="sidebar-col" class="col-lg-4">
  <div id="sidebar" class="sidebar">
      <nav aria-labelledby="flush-headingEleven"><button role="button" aria-label="close menu" alt="close menu" aria-expanded="true" aria-controls="sidebar" class="collapse-toggle" data-collapse="Collapse " data-episodes="Episodes ">
          <i class="search-icon" data-feather="x" role="img"></i>
        </button>
        <div class="sidebar-inner">
          <div class="row mobile-row" id="theme-row-mobile">
            <div class="col" id="theme-selector">
              <li class="nav-item dropdown" id="theme-button-list">
                <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
                  <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><span class="d-lg-none ms-1" id="bd-theme-text">Toggle Theme</span>
                </button>
                <ul class="dropdown-menu dropdown-menu-right" aria-labelledby="bd-theme-text">
<li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                      Light
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                      Dark
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                      <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                      Auto
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                </ul>
</li>
            </div>
          </div>
          <div class="row mobile-row">
            <div class="col">
              <div class="sidenav-view-selector">
                <div class="accordion accordion-flush" id="accordionFlush9">
                  <div class="accordion-item">
                    <h2 class="accordion-header" id="flush-headingNine">
                      <button class="accordion-button collapsed" id="instructor" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseNine" aria-expanded="false" aria-controls="flush-collapseNine">
                        <i id="eye" aria-hidden="true" class="icon" data-feather="eye"></i> Instructor View
                      </button>
                    </h2>
                    <div id="flush-collapseNine" class="accordion-collapse collapse" aria-labelledby="flush-headingNine" data-bs-parent="#accordionFlush2">
                      <div class="accordion-body">
                        <a href="../aio.html">Learner View</a>
                      </div>
                    </div>
                  </div>
<!--/div.accordion-item-->
                </div>
<!--/div.accordion-flush-->
              </div>
<!--div.sidenav-view-selector -->
            </div>
<!--/div.col -->

            <hr>
</div>
<!--/div.mobile-row -->

          <div class="accordion accordion-flush" id="accordionFlush11">
            <div class="accordion-item">

              <button id="chapters" class="accordion-button show" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseEleven" aria-expanded="false" aria-controls="flush-collapseEleven">
                <h2 class="accordion-header chapters" id="flush-headingEleven">
                  EPISODES
                </h2>
              </button>
              <div id="flush-collapseEleven" class="accordion-collapse show collapse" aria-labelledby="flush-headingEleven" data-bs-parent="#accordionFlush11">

                <div class="accordion-body">
                  <div class="accordion accordion-flush" id="accordionFlush1">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading1">
        <a href="index.html">Summary and Schedule</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush2">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading2">
        <a href="introduction.html">1. Undertsanding our 3D world</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush3">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading3">
        <a href="structure-from-motion.html">2. Structure from Motion</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush4">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading4">
        <a href="creating-3d-models-via-sfm.html">3. Creating 3D models via SfM</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush5">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading5">
        <a href="software-workflow.html">4. Software Workflow</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush6">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading6">
        <a href="technologies-for-sensing.html">5. Technologies for sensing</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush7">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading7">
        <a href="tracking-in-vr.html">6. Tracking in VR</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

                </div>
              </div>
            </div>

            <hr class="half-width">
<div class="accordion accordion-flush lesson-resources" id="accordionFlush12">
              <div class="accordion-item">
                <h2 class="accordion-header" id="flush-headingTwelve">
                  <button class="accordion-button collapsed" id="lesson-resources" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseTwelve" aria-expanded="false" aria-controls="flush-collapseTwelve">
                    RESOURCES
                  </button>
                </h2>
                <div id="flush-collapseTwelve" class="accordion-collapse collapse" aria-labelledby="flush-headingTwelve" data-bs-parent="#accordionFlush12">
                  <div class="accordion-body">
                    <ul>
<li>
                        <a href="../instructor/key-points.html">Key Points</a>
                      </li>
                      <li>
                        <a href="../instructor/instructor-notes.html">Instructor Notes</a>
                      </li>
                      <li>
                        <a href="../instructor/images.html">Extract All Images</a>
                      </li>
                      <hr>
<li><a class="dropdown-item" href="reference.html">Reference</a></li>
                    </ul>
</div>
                </div>
              </div>
            </div>
            <hr class="half-width lesson-resources">
<a href="../instructor/aio.html">See all in one page</a>


            <hr class="d-none d-sm-block d-md-none">
<div class="d-grid gap-1">

            </div>
          </div>
<!-- /div.accordion -->
        </div>
<!-- /div.sidebar-inner -->
      </nav>
</div>
<!-- /div.sidebar -->
  </div>
<!-- /div.sidebar-col -->
<!-- END:   inst/pkgdown/templates/navbar.html-->

        <!-- START: inst/pkgdown/templates/content-extra.html -->
  <div class="col-xl-8 col-lg-12 primary-content">
    <main id="main-content" class="main-content"><div class="container lesson-content">
        
        
<section id="aio-introduction"><p>Content from <a href="introduction.html">Undertsanding our 3D world</a></p>
<hr>
<p>Last updated on 2024-11-26 |

        <a href="https://github.com/karina-rodriguez/2024-VR-sensing/edit/main/episodes/introduction.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 12 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How do camera devices understand data and spaces in the
real-world?</li>
<li>What information is recorded by sensor to record objects and
environments?</li>
<li>How do this method support registering motion within a physical 3D
environments?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Have an awareness of the methods available for sensing (or gathering
data) of physical 3D environments and objects.</li>
<li>Understand the different types of sensors, in particular for
detecting motion.</li>
<li>Develop a good understanding of the Structure from motion method,
also known as photogrammetry</li>
<li>Understand the applications of these sensing technologies for
Virtual Reality and Mixed Reality systems.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<figure><img src="../fig/room-wireframe.png" alt="wireframe image room" class="figure mx-auto d-block"><div class="figcaption">Charité University Hospital - Operating Room ©
Queisner M, Pogorzhelskiy M, Remde C, Pratschke J, Sauer IM. from <a href="https://sketchfab.com/3d-models/charite-university-hospital-operating-room-9ec46c4d615a4581a235eebfb162f574" class="external-link">Sketchfab</a>
</div>
</figure><p>Tracking our physical environment is an important part of Virtual and
Mixed Reality applications.</p>
<div id="challenge-why-tracking-a-physical-environment" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-why-tracking-a-physical-environment" class="callout-inner">
<h3 class="callout-title">Challenge: Why tracking a physical
environment?</h3>
<div class="callout-content">
<p>Why tracking a physical environment is relevant to the development of
VR/MR applications</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">
  Output
  </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<p>Tracking allows the system to understand how a user is moving with
respect to their physical environment, as well as allowing the user to
seamlessly interact with digital content within the same space.</p>
</div>
</div>
</div>
</div>
<p>Tracking users is particularly relevant for VR/MR applications. It
allows to determine the position and orientation of the users; as well
as sensing and providing information regarding the physical space
surrounding the user.</p>
<iframe title="Charité University Hospital - Operating Room" frameborder="0" allowfullscreen mozallowfullscreen="true" webkitallowfullscreen="true" allow="autoplay;
fullscreen; xr-spatial-tracking" xr-spatial-tracking width="100%;" height="400px;" execution-while-out-of-viewport execution-while-not-rendered web-share src="https://sketchfab.com/models/9ec46c4d615a4581a235eebfb162f574/embed">
</iframe><p><a href="https://sketchfab.com/3d-models/charite-university-hospital-operating-room-9ec46c4d615a4581a235eebfb162f574" class="external-link">Queisner
M, Pogorzhelskiy M, Remde C, Pratschke J, Sauer IM. VolumetricOR: A New
Approach to Simulate Surgical Interventions in Virtual Reality for
Training and Education. Surg Innov. 2022 Jun;29(3):406-415. doi:
10.1177/15533506211054240. Epub 2022 Feb 9. PMID: 35137646; PMCID:
PMC9438748.</a></p>
<p>For instance, to understand how the user moves in space or seamlessly
augment content in our physical space, we need spatial information about
it.</p>
<p><img src="../fig/AdobeStock_250036555.jpeg" alt="© REDPIXEL from AdobeStock" class="figure"> An additional challenge is that
things in our space are constantly moving, and being subject to the laws
of physics.</p>
<p>Not only a localization problem, but also a measuring problem. It
requires the computer to determine the exact position of the user and of
obstacles in the surrounding environment in real time.</p>
<p>All of this needs to be computed at interactive rates or fast enough
for the system to operate.</p></section><section id="aio-structure-from-motion"><p>Content from <a href="structure-from-motion.html">Structure from Motion</a></p>
<hr>
<p>Last updated on 2024-11-26 |

        <a href="https://github.com/karina-rodriguez/2024-VR-sensing/edit/main/episodes/structure-from-motion.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 12 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<p><img src="../fig/AdobeStock_656775682.svg" style="width:60.0%" alt="icon" class="figure"> Also known as photogrammetry.</p>
<p>Both terms are used to refer to the computing process of estimating
the 3D structure of a scene from a set of 2D raster images. A
photogrammetry software receives as an <strong>input</strong> a set of
raster images of an object or environment and <strong>outputs</strong> a
3D model.</p>
<div class="section level3">
<h3 id="definition">Definition<a class="anchor" aria-label="anchor" href="#definition"></a>
</h3>
<p>Defined by the American Society for Photogrammetry and Remote
Sensing, photogrammetry is the practice of gathering reliable data about
physical objects and environments through the recording, measurement,
and interpretation of photographic images.</p>
<p>Photogrammetry is a highly favoured technique for documenting the
shape and appearance of cultural heritage objects due to its
cost-effectiveness.</p>
<p>Essentially, it entails taking 2D photos with a camera and using
specialised software to create a 3D model.</p>
<p>This process involves the software initially identifying features in
the images, followed by matching these features, and finally
reconstructing the 3D object, with or without colour.</p>
</div>
<div class="section level3">
<h3 id="how-does-it-do-it">How does it do it?<a class="anchor" aria-label="anchor" href="#how-does-it-do-it"></a>
</h3>
<p>The underlying technology is more familiar than you might think! It
is based on the same principles that the ones used by our vision system
to perceive the 3-dimensional world.</p>
<figure><img src="../fig/Yokohama4MaidsChrysanthemum.jpg" style="width:60.0%" alt="two views" class="figure mx-auto d-block"><div class="figcaption">Bernard de Go Mars under Public Domain © from
[Wikimedia] (<a href="https://commons.wikimedia.org/w/index.php?curid=25846" class="external-link uri">https://commons.wikimedia.org/w/index.php?curid=25846</a>)</div>
</figure><figure><img src="../fig/Yokohama4MaidsChrysanthemumWikiAnim.gif" style="width:60.0%" alt="two views animated" class="figure mx-auto d-block"><div class="figcaption">Bernard de Go Mars under Public Domain © from
[Wikimedia] (<a href="https://commons.wikimedia.org/w/index.php?curid=25846" class="external-link uri">https://commons.wikimedia.org/w/index.php?curid=25846</a>)</div>
</figure><p>Because of this principle, analogue photographers in the 19th century
were already producing stereoscopic and photogrammetric sets of images
of many subjects<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>.</p>
</div>
<div class="section level3">
<h3 id="does-it-work-for-everything">Does it work for everything<a class="anchor" aria-label="anchor" href="#does-it-work-for-everything"></a>
</h3>
<p>There is no restriction on the scale at which photogrammetry
operates.</p>
<p>As such, photogrammetry is performed at a variety of ranges, from
microscopic images to aerial or space images.</p>
<p><strong>Be aware</strong> that it does not mean it can be applied to
every object. There are some restrictions for deploying this technique.
For instance, it does not work well with transparent or reflective,
shiny objects. Neither does it work well with objects that move
constantly, like a live animal or a dress worn by someone.</p>
<p>While photogrammetry may not be ideal for reflective, transparent,
and moving objects, there are other solutions to address such
issues.</p>
<p>Sometimes, this includes coating an object with powder-like sprays or
accessing specialised dome-shape equipment that captures the interaction
of light with an object so that this can be replicated in the 3D
model.</p>
<!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 -->
</div>
<div class="footnotes footnotes-end-of-document">
<hr>
<ol>
<li id="fn1"><p><a href="http://www.theulegium.de/fileadmin/user_upload/Texte/Meydenb.pdf" class="external-link">Jörg
Albertz (2001) Albrecht Meydenbauer – Pioneer Of Photogrammetric
Documentation Of The Cultural Heritage. Proceedings 18th International
Symposium CIPA 2001</a><a href="#fnref1" class="footnote-back">↩︎</a></p></li>
</ol>
</div></section><section id="aio-creating-3d-models-via-sfm"><p>Content from <a href="creating-3d-models-via-sfm.html">Creating 3D models via SfM</a></p>
<hr>
<p>Last updated on 2024-11-26 |

        <a href="https://github.com/karina-rodriguez/2024-VR-sensing/edit/main/episodes/creating-3d-models-via-sfm.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 12 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<section><h2 class="section-heading" id="camera-considerations">Camera considerations<a class="anchor" aria-label="anchor" href="#camera-considerations"></a>
</h2>
<hr class="half-width">
<p>Some knowledge in digital photography is useful for photogrammetry.
In case you do not have advanced skills, it is recommended that at least
the following advice should be followed.</p>
</section><section><h2 class="section-heading" id="resolution">Resolution<a class="anchor" aria-label="anchor" href="#resolution"></a>
</h2>
<hr class="half-width">
<p>Preferably shoot in <strong>RAW</strong> and in maximal resolution.
JPG compression creates noise that should be avoided. If JPG images are
to be used, then prefer high quality JPG images.</p>
</section><section><h2 class="section-heading" id="iso-values">ISO values<a class="anchor" aria-label="anchor" href="#iso-values"></a>
</h2>
<hr class="half-width">
<p><strong>ISO</strong> values should be the <strong>lowest
possible</strong> as you want <strong>clear, sharp images</strong>
without too much noise. ISO 100 will provide good pictures without much
noise but for this you will need a tripod because longer shutter speeds
will be required. For hand-held camera you can go up to ISO 800 but this
will bring more grain to your pictures.</p>
</section><section><h2 class="section-heading" id="aperture">Aperture<a class="anchor" aria-label="anchor" href="#aperture"></a>
</h2>
<hr class="half-width">
<p><strong>Aperture</strong> value (f-number) should be <strong>high
enough</strong> so as to be able to distinguish details without having
blurred surfaces. A higher f-number means that you will get a
<strong>better depth of field</strong>. Something between f/8 and f/16
would work well.</p>
</section><section><h2 class="section-heading" id="shutter-speed">Shutter speed<a class="anchor" aria-label="anchor" href="#shutter-speed"></a>
</h2>
<hr class="half-width">
<p><strong>Shutter speed</strong> should be <strong>fast enough</strong>
to freeze images and avoid blur that is caused by the movement of the
camera. If you are using a tripod you can use slower shutter speeds. The
rule here is that anything below <strong>(slower) than 1/60 of a second
requires a tripod</strong>.</p>
</section><section><h2 class="section-heading" id="depth-of-field-and-focus">Depth of field and focus<a class="anchor" aria-label="anchor" href="#depth-of-field-and-focus"></a>
</h2>
<hr class="half-width">
<p>You should consider always a <strong>large depth of field</strong>
when possible as <strong>good focus</strong> especially on the subject
is important.</p>
<figure><img src="https://upload.wikimedia.org/wikipedia/commons/6/6f/Depth-of-field.svg" style="width:60.0%" alt="Depth-of-field" class="figure mx-auto d-block"><div class="figcaption">Depth-of-field</div>
</figure><p>Be careful to have all the important parts of the image in focus.
Automatic focus can be used when you are rotating around the object, but
you can set focus manually if you are using a turntable.</p>
<p>For a better explanation on how <strong>depth of field</strong> works
in conjunction with <strong>aperture, focal length and focus
distance</strong> you can refer to the websites mentioned below.</p>
<p><strong>Example of settings</strong>: f/8, ISO 400, shutter speed
1/30 and if light isn’t enough you can increase ISO to 800 OR lower
shutter speed to 1/15 (remember that any shutter speed that is lower
than 1/60 requires a tripod). Please note that these are just examples
and you should check exposure for every acquisition depending on current
light conditions.</p>
<p>More information:</p>
<ul>
<li>Cambridge in Color: <a href="https://www.cambridgeincolour.com/tutorials/depth-of-field.htm" class="external-link">https://www.cambridgeincolour.com/tutorials/depth-of-field.htm</a>
</li>
<li>Photographylife: <a href="https://photographylife.com/what-is-depth-of-field" class="external-link">https://photographylife.com/what-is-depth-of-field</a>
</li>
</ul></section><section><h2 class="section-heading" id="before-you-start">Before you start<a class="anchor" aria-label="anchor" href="#before-you-start"></a>
</h2>
<hr class="half-width">
<p>Before embarking on photogrammetry, consider the object you want to
digitise and the space available to do this. You might want to ask
yourself:</p>
<ul>
<li>Does the object has enough texture for the software to find enough
features?</li>
<li>If outdoors, is the building or environment in a busy area with
passers by or lots of foliage?</li>
<li>Will I use natural or artificial or natural light to best illuminate
the object?</li>
<li>What equipment will I need with me if I need to travel to perform
the digitisation, what will I have access once I am on site (e.g. power
to recharge batteries)?</li>
</ul>
<p>Overall, the recommendation is to <strong>avoid plain and monotonous
surfaces</strong>. Flat, shiny, transparent, very thin artefacts and
textures such as fur, hair won’t be the ideal candidates for
photogrammetry.</p>
<p><strong>Moving objects</strong> (e.g. leaves of a tree or people
walking) are not good candidates either.</p>
<p>Some objects are <strong>shinny</strong> and the
<strong>reflections</strong> will result in having lots of noise, hence
a ‘bad’ model. Adding talc or corn-starch on the surface of the object
could be a solution but this cannot be applied on most cultural heritage
artefacts.</p>
<p>Significant colour changes or colour designs on a relatively plain
surface could provide good reference points and help us to produce a
model.</p>
<p>The best candidates are rigid, non-reflective, textured
artefacts.</p>
</section><section><h2 class="section-heading" id="photographing-the-objectenvironment">Photographing the object/environment<a class="anchor" aria-label="anchor" href="#photographing-the-objectenvironment"></a>
</h2>
<hr class="half-width">
<p>Good acquisition of images is important in order to have a successful
project.</p>
<p>Depending on the type of the object and scene you want to acquire,
you might be able to create an optimal setup which can allow you to
develop a workflow to provide you with a good 3D model.</p>
<p>Thankfully, there is guidance and best practices, provided by
independent bodies and software companies. For example, see the <a href="https://www.3dflow.net/technology/documents/photogrammetry-how-to-acquire-pictures/" class="external-link">3D
Flow</a> software guidance. We will be using this software in this
workshop.</p>
<p>In general, start the acquisition from an angle/view of the object
that has many details and is not very plain. Thereafter, you need to
take images around the object as shown in the image below.</p>
<div class="section level3">
<h3 id="overlapping">Overlapping<a class="anchor" aria-label="anchor" href="#overlapping"></a>
</h3>
<p>In all cases you need many overlapping images of the object or
environment. When taking images, you need enough overlap between, around
50-60%, to make sure that the software will be able to align the images
correctly.</p>
</div>
<div class="section level3">
<h3 id="number-of-images">Number of images<a class="anchor" aria-label="anchor" href="#number-of-images"></a>
</h3>
<p>20-60 for each 360 acquisition. Remember that it is better to have
more images than less. ‘Bad’ images (e.g. blurred, not in focus) can be
deleted before processing.</p>
<p>Remember that you should avoid having ‘blind-zones’ and the object
should occupy the maximum possible frame area.</p>
<p>Close-up photos are allowed only to capture minor details.</p>
<figure><img src="../fig/facade.svg" alt="Best practice to photograph facades. DO NOT take a panorama image of a facade. DO walk along the facade taking overlapping pictures." class="figure mx-auto d-block"><div class="figcaption">Best practice to photograph facades. DO NOT take
a panorama image of a facade. DO walk along the facade taking
overlapping pictures.</div>
</figure><figure><img src="../fig/room.svg" alt="Best practice to photograph rooms or closed spaces. DO NOT take various panorama images of the room. DO walk around the room taking overlapping pictures." class="figure mx-auto d-block"><div class="figcaption">Best practice to photograph rooms or closed
spaces. DO NOT take various panorama images of the room. DO walk around
the room taking overlapping pictures.</div>
</figure><figure><img src="../fig/object.svg" alt="Best practice to photograph objects. DO NOT take pictures without overlap. DO walk around the object taking overlapping pictures." class="figure mx-auto d-block"><div class="figcaption">Best practice to photograph objects. DO NOT take
pictures without overlap. DO walk around the object taking overlapping
pictures.</div>
</figure>
</div>
<div class="section level3">
<h3 id="targetsmarkers">Targets/markers<a class="anchor" aria-label="anchor" href="#targetsmarkers"></a>
</h3>
<p>You can put some markers and targets on/around/underneath the object
that you want to acquire to help the software with the aligning
process.</p>
<p>To support accurate measurements of 3D data you can also place a
calibrated scale image underneath the object (or scale bars around
it).</p>
<p>You can use these two images which contain both a texture, as well as
a scale image. Follow the instructions, including printing in colour and
at 1:1 scale:</p>
<ul>
<li><a href="../files/photogrammetric_scale_noncoded_markers_medium.pdf">20
cm diameter marker image</a></li>
<li><a href="../files/Photogrammetric_scale_noncoded_markers_plus_small.pdf">12 cm
diameter marker image</a></li>
</ul>
<p>Remember that these points should remain in the same position with
respect to the object.</p>
<figure><img src="../fig/HorseMatch1Cut.jpg" style="width:100.0%" alt="12 cm Marker under wooden horse object" class="figure mx-auto d-block"><div class="figcaption">12 cm Marker under wooden horse object</div>
</figure><p>So, if you move with the camera around the object they should remain
in the same place (e.g. placed around the object). But if you are using
a turntable they should turn along with the object (e.g. placed
underneath the object).</p>
</div>
<div class="section level3">
<h3 id="lighting">Lighting<a class="anchor" aria-label="anchor" href="#lighting"></a>
</h3>
<p>Good lighting is required and occlusions should be kept to
minimum.</p>
<p>The ideal conditions for an outdoor acquisition require an
overcast/cloudy day.</p>
<p>If there is sun that creates shadows, you can use a sheet to shade
the object of interest. But for buildings, it might not be possible as
illustrated below.</p>
<figure><img src="https://upload.wikimedia.org/wikipedia/commons/b/b7/D%C3%BClmen%2C_St.-Viktor-Kirche_--_2014_--_0076.jpg" style="width:50.0%" alt="Shadows created by the sunlight on Saint Viktor of Xanten Church, Dülmen, North Rhine-Westphalia, Germany, Public domain, Dietmar Rabich , under CC BY-SA 4.0, via Wikimedia Commons" class="figure mx-auto d-block"><div class="figcaption">Shadows created by the sunlight on Saint Viktor
of Xanten Church, Dülmen, North Rhine-Westphalia, Germany, Public
domain, Dietmar Rabich , under <a href="https://creativecommons.org/licenses/by-sa/4.0" class="external-link">CC BY-SA 4.0</a>,
via <a href="https://commons.wikimedia.org/wiki/File:D%C3%BClmen,_St.-Viktor-Kirche_--_2014_--_0076.jpg" class="external-link">Wikimedia
Commons</a>
</div>
</figure><figure><img src="../fig/QueenVictoria_Photo.jpg" style="width:50.0%" alt="Overcast lighting on Statue of Queen Victoria, Brighton" class="figure mx-auto d-block"><div class="figcaption">Overcast lighting on Statue of Queen Victoria,
Brighton</div>
</figure><p>For indoor acquisition, you can use static artificial light. In this
case, lights should have the same intensity.</p>
<p>It is better to use diffused light that is projected on every surface
of the object equally.</p>
<p>Two light sources can be placed on the sides of the object at an
angle of 45 degrees and one can come from the top.</p>
<p>Shadows should be avoided as much as possible (thus you might want to
add more light sources, for example one at the back).</p>
</div>
<div class="section level3">
<h3 id="background">Background<a class="anchor" aria-label="anchor" href="#background"></a>
</h3>
<p>This should be kept simple and plain.</p>
<p>There should be high contrast between the object and the background
(e.g. dark object requires bright background).</p>
</div>
</section><section><h2 class="section-heading" id="types-of-setup">Types of setup<a class="anchor" aria-label="anchor" href="#types-of-setup"></a>
</h2>
<hr class="half-width">
<p>There are two different types of setup which you can use depending on
the equipment and accesories you have access to. Their difference is
whether the camera or the object remains static.</p>
<p>In cases where you cannot touch the object, then you will only have
one choice: to move the camera.</p>
<p>In all cases you can create various series of images by varying the
height of your camera. For this, you can raise or lower as well as
adjust the angle of the camera to take another series of photos.</p>
<div class="section level3">
<h3 id="static-camera-move-object">Static camera / move object<a class="anchor" aria-label="anchor" href="#static-camera-move-object"></a>
</h3>
<p>In this setup the camera is going to remain static on a tripod.
Ideally you want to control the camera remotely to avoid small movements
which can be caused by your hand pushing the trigger.</p>
<p><img src="../fig/staticcam.svg" alt="Static camera / move object" class="figure">{alt=“static camera”Before you
start}</p>
<p>The object is placed on a turntable. A turntable which is controlled
remotely works best. Again, you want to minimise movements caused
unintentionally on the object’s position.</p>
<p>To avoid the software getting confused on the object moving but the
background remaining static, you can use a box or cloth as a
background.</p>
<p>Black or white background work better as you want to avoid
reflections onto the object and alter its colour. Black is always a safe
choice, unless the object is dark. In such case, it is best to use a
white background.</p>
<p>We will later mask the background, so that the software ignores it.
To help with this, take an image of the setup without the object before
placing the object. This will later become useful when applying the mask
to the images.</p>
<figure><img src="../fig/Box_PhotoMask.jpg" style="width:60.0%" alt="Photo of setup without object." class="figure mx-auto d-block"><div class="figcaption">Photo of setup without object.</div>
</figure><p>If artificial light is used, this should be diffused and should not
create shadows.</p>
<figure><img src="../fig/1024px-Light_Tent_DIY.jpg" style="width:60.0%" alt="turntable" class="figure mx-auto d-block"><div class="figcaption">Diffused box for static objecst</div>
</figure><figure><img src="https://upload.wikimedia.org/wikipedia/commons/d/d3/Balkan_Heritage_Field_School-5.jpg" style="width:60.0%" alt="Balkan Heritage Field School (photogrammetry course) at Stobi, Republic of Macedonia, Ivan.giogio, under CC BY-SA 4.0, via Wikimedia Commons" class="figure mx-auto d-block"><div class="figcaption">Balkan Heritage Field School (photogrammetry
course) at Stobi, Republic of Macedonia, Ivan.giogio, under <a href="https://creativecommons.org/licenses/by-sa/4.0" class="external-link">CC BY-SA 4.0</a>,
via <a href="https://commons.wikimedia.org/wiki/File:Balkan_Heritage_Field_School-5.jpg" class="external-link">Wikimedia
Commons</a>
</div>
</figure><p>The camera should be placed at a height that allows to see all
important features of the artefact (e.g. at an angle of 45 degrees above
the object).</p>
<p>When you photograph, rotate the turntable in small increments.</p>
<p>The advantage of this method is that you can have lower ISO and
shutter speeds and thus sharper images (especially in indoor
environments).</p>
<figure><img src="../fig/Horse_Low_TexturedPhotos.jpg" style="width:100.0%" alt="As the visualisation shows, the digitial photographs in this setup have a repetitive structure." class="figure mx-auto d-block"><div class="figcaption">As the visualisation shows, the digitial
photographs in this setup have a repetitive structure.</div>
</figure><p>Here you can also find a <a href="https://www.youtube.com/watch?v=Fj7wGGXPM0A" class="external-link"><strong>video</strong></a>
of a DIY rig that aims to speed up the process when shooting small and
medium objects by <a href="https://en.openscan.eu/" class="external-link"><strong>Openscan.eu</strong></a>.</p>
</div>
<div class="section level3">
<h3 id="static-object-move-camera">Static object / move camera<a class="anchor" aria-label="anchor" href="#static-object-move-camera"></a>
</h3>
<p>The object is placed at the centre and the photographer moves around
it taking pictures.</p>
<figure><img src="../fig/staticobj.svg" style="width:60.0%" alt="static object" class="figure mx-auto d-block"><div class="figcaption">Static object / move camera</div>
</figure><p>Place the item at a good height so that it is possible to take images
from a higher and a lower level.</p>
<p>In case some areas are not that visible, remember to take different
pictures of that part from different angles.</p>
<p>The advantage of this method is that it will allow you to acquire
larger objects without setting up lights.</p>
<figure><img src="../fig/objstaticphotos.png" alt="static obj" class="figure mx-auto d-block"><div class="figcaption">As the visualisation shows, the digitial
photographs in this setup are more organic, as the photographer moves
taking more or less pictures where required.</div>
</figure><div id="challenge-taking-photos" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="challenge-taking-photos" class="callout-inner">
<h3 class="callout-title">Challenge: Taking photos</h3>
<div class="callout-content">
<p><strong><em>This activity can be done as a team</em></strong></p>
<p>Experiment with both setups for photographing one or more objects
relevant to your project. When doing this consider the best practices,
including:</p>
<ul>
<li>Best camera and area setup.</li>
<li>Adding scale to the images.</li>
<li>How many series of images you plan to acquire and what is the
strategy to achieve this.</li>
</ul>
</div>
</div>
</div>
<!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 -->
</div>
</section></section><section id="aio-software-workflow"><p>Content from <a href="software-workflow.html">Software Workflow</a></p>
<hr>
<p>Last updated on 2024-11-26 |

        <a href="https://github.com/karina-rodriguez/2024-VR-sensing/edit/main/episodes/software-workflow.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 90 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<section><h2 class="section-heading" id="step-1-preparing-the-images">Step 1: Preparing the images<a class="anchor" aria-label="anchor" href="#step-1-preparing-the-images"></a>
</h2>
<hr class="half-width">
<p>Once the photographs have been acquired, the next step is to transfer
the images to a PC.</p>
<p>Using a batch processing software, such as <a href="https://www.rawtherapee.com/" class="external-link">Raw Therapee</a>, you can convert
the images from the raw file format to a format supported by the
photogrammetry software. If you were not shooting using RAW settings,
you can skip this step.</p>
<p>Usually, the uncompressed TIFF or TIF file format is a good choice,
as it is uncompressed retaining a good quality. JPEGs have a compressed
format, and although faster to handle might provide less data for
feature recognition.</p>
<p>In any case, you must be sure you choose a format that retains the
EXIF information within the file. This is because it contains useful
information for the software projecting feature detected into 3D
points.</p>
<p>You can check EXIF information online, for example using <a href="https://exifinfo.org/" class="external-link">ExifInfo.org</a>.</p>
<p>For this lesson, we provide images in [TIFF and JPEG
format][datasets].</p>
</section><section><h2 class="section-heading" id="step-2-organising-your-workspace">Step 2: Organising your workspace<a class="anchor" aria-label="anchor" href="#step-2-organising-your-workspace"></a>
</h2>
<hr class="half-width">
<p>We will start by creating a structure to store all the files for your
project.</p>
<p>Create a folder using a name which reflects your project.</p>
<p>Good practices include:</p>
<pre><code><span>    <span class="va">ResourceIDofObjectifExistent_NameofObject_DateModelCreatedYY.MM.DD</span></span></code></pre>
<p>Transfer all images into a folder.</p>
<p>Within this folder, create another folder named <em>images</em>.</p>
<p>Copy the images from the camera into the <em>images</em> folder.</p>
</section><section><h2 class="section-heading" id="step-3-create-a-3df-zephyr-project">Step 3: Create a 3DF Zephyr project<a class="anchor" aria-label="anchor" href="#step-3-create-a-3df-zephyr-project"></a>
</h2>
<hr class="half-width">
<p>The following instructions are specific to <a href="https://www.3dflow.net/3df-zephyr-photogrammetry-software/" class="external-link">3DF
Zephyr</a>.</p>
<p>Go to the workflow menu and choose <strong>New Project</strong>, you
will be presented with a <em>“New project wizard window”</em>.</p>
<p>Choose the first box <em>Sparse</em> in order to go through the whole
process manually.</p>
<p>Click <strong>Next&gt;</strong> you will be presented with the
<em>“Photos selection page”</em>.</p>
<p>Browse to the folder that contains your images and click
<strong>Select Folder</strong> or select the relevant images if using
<strong>Single Images</strong>.</p>
<p>Click <strong>Next&gt;</strong></p>
<p>You will be presented with the <em>“Camera calibration
page</em>”.</p>
<p>If you have a separate Exif file for calibrating the camera you can
add it here, and you can also manually calibrate your camera in the
<em>“Modify Calibration page”</em> otherwise go on and click
<strong>Next&gt;</strong></p>
<figure><img src="../fig/Box_Photo.jpg" alt="photo" class="figure mx-auto d-block"><div class="figcaption">Image from photogrammetry acquistion.</div>
</figure></section><section><h2 class="section-heading" id="step-4-importing-masks-optional">Step 4: Importing masks (optional)<a class="anchor" aria-label="anchor" href="#step-4-importing-masks-optional"></a>
</h2>
<hr class="half-width">
<p>Masking allows the software to concentrate in the most important
information which is the object you have acquired. It works best when
using the “static camera / object moves” setup.</p>
<p>In the <em>“Photos selection page”</em> there is an option to import
the mask.</p>
<p>If selected a new option will be presented and a new tool called
<strong>Masquerade</strong> will be available before importing the
images.</p>
<p>Within this tool (which is also available from the main interface),
it will be possible to generate a Mask to apply to all the images.</p>
<p>The tool is simple to use. To create a mask, you can use an image
without any object on the turntable if using such setup.</p>
<figure><img src="../fig/Box_PhotoMask.jpg" alt="Example of image showing the mask which is loaded into the software" class="figure mx-auto d-block"><div class="figcaption">Example of image showing the mask which is
loaded into the software</div>
</figure><p>But you can use a sample image provided in the dataset as a first
file.</p>
</section><section><h2 class="section-heading" id="step-5-aligning-photos">Step 5: Aligning photos<a class="anchor" aria-label="anchor" href="#step-5-aligning-photos"></a>
</h2>
<hr class="half-width">
<p>The next step is to align the photos.</p>
<p>This step will perform the three substeps: feature detection, feature
matching and the first stage of the structure reconstruction step.</p>
<p>In the software, you will be presented with the <em>“Camera
orientation page”</em>. Keep the general setting and click
<strong>Next&gt;</strong>.</p>
<p>More details about how to manipulate these and the following steps by
selecting specific parameters can be found with more detail in the <a href="https://www.3dflow.net/zephyr-doc/en/Cover.html" class="external-link">software online
manual</a>.</p>
<p>To start the reconstruction, click <strong>Run</strong> in the
<em>“Start reconstruction”</em> page.</p>
<p>You will be presented with the <em>“Reconstruction Successful
page”</em>. Click <strong>Finish</strong>.</p>
<p>Save the project in your project folder.</p>
<p>Once the camera orientation phase has been completed, the sparse
point cloud will appear in the workspace as well as the oriented cameras
identified by blue pyramids.</p>
<p>Now you can familiarise yourself with the navigation of the 3D space
and the interface.</p>
<p>For example, go to <strong>Scene &gt; Bounding Box &gt; Edit Bounding
box</strong> and limit the created sparse cloud within the bounding
box.</p>
<p>This will speed up the process when creating the final mesh.</p>
<figure><img src="../fig/Box_High_PointCloud.jpg" alt="pointcloud" class="figure mx-auto d-block"><div class="figcaption">Sparse point cloud generated by 3DFlow ZEPHYR
software</div>
</figure></section><section><h2 class="section-heading" id="step-6-build-a-dense-point-cloud">Step 6: Build a dense point cloud<a class="anchor" aria-label="anchor" href="#step-6-build-a-dense-point-cloud"></a>
</h2>
<hr class="half-width">
<p>Go to<strong>Workflow</strong>in the menu and select <strong>Advanced
&gt; Dense Point Cloud Generation</strong>.</p>
<p>You will be presented with the <em>“Dense Point Cloud Generation
wizard”</em>. <strong>Select All Cameras</strong> and click
<strong>Next&gt;</strong></p>
<p>When presented with the <em>“Dense Point Cloud Creation”</em> page,
leave the general settings and click <strong>Next&gt;</strong></p>
<p>Click <strong>Run</strong> on the<em>“Start Densification”</em>
page.</p>
<p>When finished, you will see the <em>“Dense Point Cloud generation
successful”</em> page. Click <strong>Finish</strong>.</p>
<p>Save the project.</p>
<figure><img src="../fig/Box_DensePointCloud.jpg" alt="Dense point cloud generated by 3DFlow ZEPHYR software" class="figure mx-auto d-block"><div class="figcaption">Dense point cloud generated by 3DFlow ZEPHYR
software</div>
</figure></section><section><h2 class="section-heading" id="step-7-cleaning-the-dense-cloud">Step 7: Cleaning the dense cloud<a class="anchor" aria-label="anchor" href="#step-7-cleaning-the-dense-cloud"></a>
</h2>
<hr class="half-width">
<p>Before trying to create the final mesh, it is useful to delete all
the unwanted points. The same bounding box can be used, or the unwanted
points can be deleted manually.</p>
<p>Go to the <em>Editing panel</em> on your right and choose <strong>By
Hand</strong>. Choose <strong>Poly</strong> and
<strong>Remove</strong>.</p>
<p>Start selecting the points that you do not need and once selected
deleted them with the delete key.</p>
<p>Once happy save the project.</p>
</section><section><h2 class="section-heading" id="step-8-building-the-3d-model">Step 8: Building the 3D model<a class="anchor" aria-label="anchor" href="#step-8-building-the-3d-model"></a>
</h2>
<hr class="half-width">
<p>Go to<strong>Workflow</strong>in the menu and select <strong>Advanced
&gt; Mesh Extraction</strong></p>
<p>You will be presented with the <em>“Mesh Generation wizard”</em>.</p>
<p>Select from the <em>drop down</em> the name of your dense point
cloud. <strong>Select All Cameras</strong> and click
<strong>Next&gt;</strong></p>
<p>Leave the general settings on the <em>“Surface Reconstruction”</em>
page and click <strong>Next&gt;</strong></p>
<p>Click <strong>Run</strong> on the<em>“Start Mesh Creation”</em>
page.</p>
<p>When finished you will see the <em>“Mesh Creation successful”</em>
page. Click <strong>Finish</strong>. This process will produce a 3D
model.</p>
<p>Save the project.</p>
<figure><img src="../fig/Box_High_Wireframe.jpg" style="width:50.0%" alt="3D model" class="figure mx-auto d-block"><div class="figcaption">3D model generated by 3DFlow ZEPHYR
software</div>
</figure></section><section><h2 class="section-heading" id="step-9-building-the-raster-texture">Step 9: Building the raster texture<a class="anchor" aria-label="anchor" href="#step-9-building-the-raster-texture"></a>
</h2>
<hr class="half-width">
<p>The final step is to re-project the texture onto the 3D surface.</p>
<p>Go to<strong>Workflow</strong>in the menu and select <strong>Textured
Mesh Generation</strong>.</p>
<p>You will be presented with the <em>“Textured Mesh Generation
wizard”</em>. In the <em>drop down</em> menu select the name of your
mesh. <strong>Select All Cameras</strong> and click
<strong>Next&gt;</strong></p>
<p>Leave the general settings in the <em>“Texturing”</em> page and click
<strong>Next&gt;</strong></p>
<p>Click <strong>Run</strong> on the <em>“Textured Mesh Generation
wizard”</em> page.</p>
<p>When finished you will see the <em>“Textured Mesh Generation wizard
result”</em> page. Click <strong>Finish</strong>.</p>
<p>Now you will have a 3D model with the texture.</p>
<p>Save the project.</p>
<p><img src="../fig/BoxTexture_Low.jpg" alt="texture" class="figure"><img src="../fig/Box_High_Textured.jpg" class="figure"></p>
</section><section><h2 class="section-heading" id="step-10-exporting-the-3d-model">Step 10: Exporting the 3D model<a class="anchor" aria-label="anchor" href="#step-10-exporting-the-3d-model"></a>
</h2>
<hr class="half-width">
<p>At this point, we can export the 3D model.</p>
<p>Create another folder called <em>“Exports”</em> within your project
folder and save the model in this folder.</p>
<p>It will be useful to export various 3D models at various resolutions.
This will allow you to keep a high-resolution 3D model, while having a
low-resolution 3D model for sharing and making available on the web.</p>
<p>It is good practice to name your models with information, for
example:</p>
<pre><code><span>    <span class="va">ResourceIDofObjectifExistent_NameofObject_DateModelCreatedYY.MM.DD.</span><span class="op">[</span><span class="va">highres</span><span class="op">|</span><span class="va">medres</span><span class="op">|</span><span class="va">lowres</span><span class="op">]</span></span></code></pre>
<p>Go to <strong>Export</strong> in the menu and select <strong>Export
Textured Mesh</strong>.</p>
<p>Select from the <em>drop down</em>, the name of your mesh.</p>
<p>Select from the <em>drop down</em> your preferred format and click
<strong>Export</strong>.</p>
<p>To export the model at a lower resolution select your textured mesh
in the right window <em>“Textured Meshes”</em>. <strong>Right
Click</strong> on it and select <strong>Clone</strong>. A copy of your
mesh will be created.</p>
<p>Go to <strong>Tools</strong> in the menu and select <strong>Mesh
Filters &gt; Decimation</strong>. You will be presented with the
<em>“Mesh decimation”</em> window.</p>
<p>Select in the <em>drop down</em> menu the name of your cloned mesh.
Select <strong>preserve boundaries</strong> and <strong>Apply
Filter</strong>.</p>
<p>At this point, we need to regenerate the texture for the
lower-resolution mesh. To do so we need to repeat the process above for
generating the texture.</p>
<p>Go to <strong>Export</strong> in the menu and select <strong>Export
Textured Mesh</strong>. Select in the <em>drop down</em> menu the name
of your second mesh.</p>
<p>For web sharing GLTF is a good format to use. Select from <em>drop
down</em> menu the format <strong><em>.glb</em></strong> or
<strong><em>.gltf</em></strong> and click <strong>Export</strong>.</p>
<iframe src="https://gltf-viewer.donmccurdy.com#kiosk=1&amp;model=https://data.d4science.org/shub/E_azJzMVp6MENORnRUd0FEdElCa3g5WVBIdEQ5cldBUlJwOHkyYjRITHpTYmVUcFdIUDc1VzRhWTFGdWc5SytNVA==" style="width: 100%;" height="400px" bgcolor="#dbdbdb" frameborder="0">
</iframe>
<div id="challenge-processing-the-photos-and-creating-a-3d-model" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="challenge-processing-the-photos-and-creating-a-3d-model" class="callout-inner">
<h3 class="callout-title">Challenge: Processing the photos and creating
a 3D model</h3>
<div class="callout-content">
<p><strong><em>This activity can be done as a team</em></strong></p>
<p>By using the images you captured during the lesson, use the software
to create a 3D model.</p>
<p>Produce two versions: a high-resolution and a low-resolution and save
on your PC.</p>
</div>
</div>
</div>
<!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 -->
</section></section><section id="aio-technologies-for-sensing"><p>Content from <a href="technologies-for-sensing.html">Technologies for sensing</a></p>
<hr>
<p>Last updated on 2024-11-26 |

        <a href="https://github.com/karina-rodriguez/2024-VR-sensing/edit/main/episodes/technologies-for-sensing.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 12 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<section><h2 class="section-heading" id="sensing-data">Sensing data<a class="anchor" aria-label="anchor" href="#sensing-data"></a>
</h2>
<hr class="half-width">
<figure><img src="../fig/AdobeStock_25679833.jpeg" alt="sensors" class="figure mx-auto d-block"><div class="figcaption">Microchip board with sensor © Kadmy from
AdobeStock</div>
</figure><p>Sensors <strong>sample signals</strong> that measure real world
physical data. This includes sampling:</p>
<ul>
<li>Acceleration forces (accelerometer)</li>
<li>Visible light (photosensors)</li>
<li>Images (CCD sensor)</li>
<li>Distances to objects (laser based detection)</li>
<li>Other data including temperature, humidity, pressure, wind direction
and speed, illumination intensity, vibration intensity, sound intensity,
power-line voltage, chemical concentrations, pollutant levels and vital
body functions.</li>
</ul>
<p><strong>Samples</strong> are converted into numeric values that can
be manipulated by a computer.</p>
<p>Driven by the low cost, many of the devices which we use today
contain sensors. This allows us to sense data all around us.Most of
these sensors are relevant for <a href="https://www.youtube.com/watch?v=DidxdOAkpwA" class="external-link">Internet of
Things</a>, and Internet of Place technologies.</p>
<p><a href="https://www.youtube.com/watch?v=DidxdOAkpwA" title="IoT technologies" class="external-link"><img src="https://i.ytimg.com/vi/DidxdOAkpwA/maxresdefault.jpg" alt="IMAGE ALT TEXT" class="figure"></a></p>
<p>Sensors also allows objects and environments to respond and interact
with their environment.</p>
<p>Building on their wide availability, sensors also allow Virtual
Reality and Mixed Reality to be more affordable.</p>
<div id="challenge-vr-sensors" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="challenge-vr-sensors" class="callout-inner">
<h3 class="callout-title">Challenge: VR sensors?</h3>
<div class="callout-content">
<p>Think on which sensors are used by VR headests?</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<p>VR headset can include a variety of sensors, including:</p>
<ul>
<li>Inertial Measurement Unit (IMU) to measures force and angular rate
by using a gyroscope, accelerometer and magnetometer.</li>
<li>Image sensor</li>
<li>Proximity sensor</li>
</ul>
</div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="motion-capture">Motion capture<a class="anchor" aria-label="anchor" href="#motion-capture"></a>
</h2>
<hr class="half-width">
<p>Detecting motion is of interest to many applications, including the
film and game industry. Sensors can detect any moving objects, and for
many years they have been used fro detecting movement of people’s
bodies.</p>
<p>This is in particular relevant for animation of digital data. It
allows to transfers the movement of an actor to a digital character.</p>
<p>Requires to have a 3D polygonal mesh which is ready for
animation.</p>
<figure><img src="https://upload.wikimedia.org/wikipedia/commons/8/8b/Motion_Capture_with_Chad_Phantom.png" alt="3d model" class="figure mx-auto d-block"><div class="figcaption">Real-life motion data (left) is acquired on a
motion capture platform (center) and used to determine the posture of
the CHAD phantom (right) © Vazquez88</div>
</figure><p>However, remember polygonal meshes are RIGID. They do cannot be
easily deformed. For this, animators undertake a process to
<strong>animated</strong> or transform sections of the polygonal
mesh.</p>
<p>This requires creating a system of <em>joints</em> (similar than our
human body). This process is known as rigging. A rig is the digital
skeleton formed of joints and bones.</p>
<p>This functionality is offered by modelling packages, and game
engines:</p>
<ul>
<li>Unity: <a href="https://docs.unity3d.com/Manual/AnimationSection.html" class="external-link uri">https://docs.unity3d.com/Manual/AnimationSection.html</a>
</li>
<li>Unreal: <a href="https://dev.epicgames.com/documentation/en-us/unreal-engine/animating-characters-and-objects-in-unreal-engine" class="external-link uri">https://dev.epicgames.com/documentation/en-us/unreal-engine/animating-characters-and-objects-in-unreal-engine</a>
</li>
<li>Godot: <a href="https://docs.godotengine.org/en/stable/tutorials/animation/introduction.html" class="external-link uri">https://docs.godotengine.org/en/stable/tutorials/animation/introduction.html</a>
</li>
</ul>
<figure><img src="https://upload.wikimedia.org/wikipedia/commons/9/97/Two_repetitions_of_a_walking_sequence_of_an_individual_recorded_using_a_motion-capture_system.gif" alt="motion" class="figure mx-auto d-block"><div class="figcaption">Two repetitions of a walking sequence recorded
using a motion-capture system. The spatial trajectories of limb
movements are highly similar despite of the timing of movement differing
between repetitions. Data are presented and analyzed in the paper
Simultaneous inference for misaligned multivariate functional data ©
Lars Lau Raket</div>
</figure><p>Below you will find details on the different technologies for
capturing data for animation.</p>
</section><section><h2 class="section-heading" id="types-of-tracking-systems-for-motion-capture">Types of Tracking Systems for Motion Capture<a class="anchor" aria-label="anchor" href="#types-of-tracking-systems-for-motion-capture"></a>
</h2>
<hr class="half-width">
<p>There are different type of systems: optical vs non optical
systems.</p>
<div class="section level3">
<h3 id="non-optical-tracking">Non-optical tracking<a class="anchor" aria-label="anchor" href="#non-optical-tracking"></a>
</h3>
<ul>
<li>Based on sensors which measure inertia or mechanical motion,
including accelerometer electromechanical device that measure
acceleration forces.</li>
<li>Come often with easy wearable systems.</li>
<li>Enable accurate motion capture.</li>
</ul>
<p>For example, <a href="https://www.youtube.com/watch?v=-0hSQFbt67U&amp;t=1s" class="external-link">Xsens MVN
solution for 3D Character Animation</a>.</p>
<p><a href="https://www.youtube.com/watch?v=-0hSQFbt67U" title="3D chracater" class="external-link"><img src="https://i.ytimg.com/vi/-0hSQFbt67U/maxresdefault.jpg" alt="Xsens MVN solution for 3D Character Animation" class="figure"></a></p>
</div>
<div class="section level3">
<h3 id="optical-tracking">Optical tracking<a class="anchor" aria-label="anchor" href="#optical-tracking"></a>
</h3>
<ul>
<li>Use multiple digital cameras.</li>
<li>Based on the information provided by the cameras looking at the
element in motion-tracked within a limited area.</li>
<li>Use position markers in the environment.</li>
<li>Assemble the data into an approximation of the actor’s motion.</li>
<li>The specific technology is based on the role light plays in the
capture process: active versus passive system.</li>
</ul>
<p><a href="https://www.youtube.com/watch?v=O0mLfzbmqcg" class="external-link"><img src="https://i.ytimg.com/vi/O0mLfzbmqcg/maxresdefault.jpg" alt="ALT TEXT" class="figure"></a></p>
</div>
<div class="section level3">
<h3 id="active-tracking">Active tracking<a class="anchor" aria-label="anchor" href="#active-tracking"></a>
</h3>
<ul>
<li>Markers are based on light such as LEDs.</li>
<li>Systems illuminate one LED or multiple LEDs at a time.</li>
<li>Software identify markers by their relative positions.</li>
</ul>
</div>
<div class="section level3">
<h3 id="passive-tracking">Passive tracking<a class="anchor" aria-label="anchor" href="#passive-tracking"></a>
</h3>
<p>Based on information provided by one or more digital cameras,
including depth sensors.</p>
<p>This allows for example hand and finger tracking</p>
<p>VR devices now come with depth camera system for finger tracking.</p>
<p><a href="https://www.youtube.com/watch?v=rnlCGw-0R8g" class="external-link"><img src="https://i.ytimg.com/vi/rnlCGw-0R8g/maxresdefault.jpg" alt="ALT TEXT" class="figure"></a></p>
<div class="section level4">
<h4 id="hybrid-systems">Hybrid systems<a class="anchor" aria-label="anchor" href="#hybrid-systems"></a>
</h4>
<ul>
<li>Use accelerometers and images.</li>
<li>Features in the images are used as markers.</li>
<li>Similar to Structure from Motion technology.</li>
</ul>
<p>Augmented Reality Systems make use of hybrid systems for <a href="https://www.youtube.com/watch?v=2y7NX-HUlMc&amp;t=301s" class="external-link">tracking
capabilities</a>. <a href="https://www.youtube.com/watch?v=2y7NX-HUlMc" class="external-link"><img src="https://i.ytimg.com/vi/2y7NX-HUlMc/maxresdefault.jpg" alt="ALT TEXT" class="figure"></a></p>
</div>
</div>
</section><section><h2 class="section-heading" id="advantages-and-disadvantages">Advantages and disadvantages<a class="anchor" aria-label="anchor" href="#advantages-and-disadvantages"></a>
</h2>
<hr class="half-width">
<p>Non-optical systems are portable but can restrict movement.</p>
<p>Optical systems can be very precise, but are non portable.</p>
</section><section><h2 class="section-heading" id="performance-and-cost">Performance and cost<a class="anchor" aria-label="anchor" href="#performance-and-cost"></a>
</h2>
<hr class="half-width">
<p>An important metric for sensing is the accuracy of the system. This
includes both hardware and software.</p>
<p>Active methods are more accurate, depending on the emitting power of
the light source. But they tend to have a higher cost.</p>
<p>Passive methods are reliant on the ability to find features in the
scene, but have a lower cost. <!--
## Image sensors

In a camera system,
the image sensor receives incident light (photons)
and transform it into a digital image.


Active and passive methods.

Active methods provide their own source of energy to illuminate the objects they observe.

For instance LIDAR (Light Detection and Ranging), depth sensor and 3D scanners.
--></p>
<!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 -->
</section></section><section id="aio-tracking-in-vr"><p>Content from <a href="tracking-in-vr.html">Tracking in VR</a></p>
<hr>
<p>Last updated on 2024-11-26 |

        <a href="https://github.com/karina-rodriguez/2024-VR-sensing/edit/main/episodes/tracking-in-vr.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 12 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<p>Virtual Reality (VR) headsets are distinct from regular 3D displays
in that they are tracked. This allows the system to provide 3 Degrees or
6 Degrees of Freedom.</p>
<figure><img src="https://upload.wikimedia.org/wikipedia/commons/3/33/Positional_tracking_in_virtual_reality.png" alt="positional tracking" class="figure mx-auto d-block"><div class="figcaption">Positional tracking in virtual reality ©
Ilsladkih at <a href="https://commons.wikimedia.org/wiki/File:Positional_tracking_in_virtual_reality.png" class="external-link">Wikimedia</a>
</div>
</figure><p>Tracking in VR is also known as a positional tracking or pose
tracking. It allows the headset to capture, follow, and get information
about an object’s orientation and position, to be transferred to an
application for further processing.</p>
<figure><img src="https://developers.google.com/static/ml-kit/images/vision/pose-detection/jump.gif" alt="pose tracking" class="figure mx-auto d-block"><div class="figcaption">Pose estimation library from Google © <a href="https://developers.google.com/ml-kit/vision/pose-detection" class="external-link">Google</a>
</div>
</figure><p>See examples of Pose Estimation libraries:</p>
<ul>
<li>Media Pipe: <a href="https://huggingface.co/spaces/hysts/mediapipe-pose-estimation" class="external-link">https://huggingface.co/spaces/hysts/mediapipe-pose-estimation</a>
</li>
<li>Tensor Flow: <a href="https://www.tensorflow.org/lite/examples/pose_estimation/overview" class="external-link">https://www.tensorflow.org/lite/examples/pose_estimation/overview</a>
Pose detection</li>
</ul>
<figure><img src="https://storage.googleapis.com/download.tensorflow.org/example_images/movenet_demo.gif" alt="Pose estimation library from Tensor Flow © Tensor Flow" class="figure mx-auto d-block"><div class="figcaption">Pose estimation library from Tensor Flow © <a href="https://www.tensorflow.org/lite/examples/pose_estimation/overview" class="external-link">Tensor
Flow</a>
</div>
</figure><section><h2 class="section-heading" id="what-do-we-track">What do we track?<a class="anchor" aria-label="anchor" href="#what-do-we-track"></a>
</h2>
<hr class="half-width">
<p>In a VR system, the aim is to track information of the user,
including:</p>
<ul>
<li>Position and orientation of the user’s head.</li>
<li>Controllers and other important objects.</li>
<li>Hands</li>
</ul>
<p>In AR systems, object, images, or markers are used to determine the
user’s position and orientation.</p>
</section><section><h2 class="section-heading" id="types-of-tracking">Types of tracking<a class="anchor" aria-label="anchor" href="#types-of-tracking"></a>
</h2>
<hr class="half-width">
<p>As before, tracking can be optical vs non optical. We will focus on
the optical, and make a distinction between outside-in and inside-out
tracking.</p>
</section><section><h2 class="section-heading" id="optical-outside-in-tracking">Optical outside-in tracking<a class="anchor" aria-label="anchor" href="#optical-outside-in-tracking"></a>
</h2>
<hr class="half-width">
<p>In this type of tracking, cameras are placed in stationary locations
in the environment to track the position of markers on the tracked
device. For example, the HTC VIVE uses outside-in tracking.</p>
<p>IR LEDs on its headset and controllers allow external cameras in the
environment to read their positions.</p>
</section><section><h2 class="section-heading" id="optical-inside-out-tracking">Optical Inside-out tracking<a class="anchor" aria-label="anchor" href="#optical-inside-out-tracking"></a>
</h2>
<hr class="half-width">
<p>The camera is placed on the tracked device and looks outward to
determine its location in the environment.</p>
<p>Headsets have multiple cameras facing different directions to get
views of its entire surroundings.</p>
<p>Can work with or without markers.</p>
</section><section><h2 class="section-heading" id="slam-simultaneous-localization-and-mapping">SLAM (Simultaneous localization and mapping)<a class="anchor" aria-label="anchor" href="#slam-simultaneous-localization-and-mapping"></a>
</h2>
<hr class="half-width">
<p>Markerless tracking, such as on the Oculus Quest.</p>
<p>Algorithms to construct or update a map of an unknown environment
while simultaneously keeping track of an agent’s location within it</p>
<p>A 3D map of the environment is generated in real time.</p>
<p><a href="https://www.youtube.com/watch?app=desktop&amp;v=J5oW7r-2dlM" title="tracking" class="external-link"><img src="https://i.ytimg.com/vi/J5oW7r-2dlM/maxresdefault.jpg" alt="IMAGE ALT TEXT" class="figure"></a></p>
<p>Further information: <a href="https://xinreality.com/wiki/Main_Page" class="external-link uri">https://xinreality.com/wiki/Main_Page</a></p>
<!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 -->
</section></section>
</div>
    </main>
</div>
<!-- END  : inst/pkgdown/templates/content-extra.html -->

      </div>
<!--/div.row-->
      		<footer class="row footer mx-md-3"><hr>
<div class="col-md-6">
        <p>This lesson is subject to the <a href="CODE_OF_CONDUCT.html">Code of Conduct</a></p>
        <p>

        <a href="https://github.com/karina-rodriguez/2024-VR-sensing/edit/main/README.md" class="external-link">Edit on GitHub</a>

	
        | <a href="https://github.com/karina-rodriguez/2024-VR-sensing/blob/main/CONTRIBUTING.md" class="external-link">Contributing</a>
        | <a href="https://github.com/karina-rodriguez/2024-VR-sensing/" class="external-link">Source</a></p>
				<p><a href="https://github.com/karina-rodriguez/2024-VR-sensing/blob/main/CITATION" class="external-link">Cite</a> | <a href="mailto:K.rodriguez@brigthon.ac.uk">Contact</a> | <a href="https://carpentries.org/about/" class="external-link">About</a></p>
			</div>
			<div class="col-md-6">

        <p>Materials licensed under <a href="LICENSE.html">CC-BY 4.0</a> by the authors</p>

        <p>Template licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/" class="external-link">CC-BY 4.0</a> by <a href="https://carpentries.org/" class="external-link">The Carpentries</a></p>
        <p>Built with <a href="https://github.com/carpentries/sandpaper/tree/0.16.10" class="external-link">sandpaper (0.16.10)</a>, <a href="https://github.com/carpentries/pegboard/tree/0.7.7" class="external-link">pegboard (0.7.7)</a>, and <a href="https://github.com/carpentries/varnish/tree/1.0.5" class="external-link">varnish (1.0.5)</a></p>
			</div>
		</footer>
</div> <!-- / div.container -->
	<div id="to-top">
		<a href="#top">
      <i class="search-icon" data-feather="arrow-up" role="img" aria-label="Back To Top"></i><br><!-- <span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top --><span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top
		</a>
	</div>
  <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "TrainingMaterial",
  "@id": "https://karina-rodriguez.github.io/2024-VR-sensing/instructor/aio.html",
  "inLanguage": "en",
  "dct:conformsTo": "https://bioschemas.org/profiles/TrainingMaterial/1.0-RELEASE",
  "description": "A Carpentries Lesson teaching foundational data and coding skills to researchers worldwide",
  "keywords": "photogrammetry, 3D digitisation, The Carpentries",
  "name": "All in One View",
  "creativeWorkStatus": "active",
  "url": "https://karina-rodriguez.github.io/2024-VR-sensing/instructor/aio.html",
  "identifier": "https://karina-rodriguez.github.io/2024-VR-sensing/instructor/aio.html",
  "dateCreated": "2024-11-19",
  "dateModified": "2024-11-26",
  "datePublished": "2024-11-26"
}

  </script><script>
		feather.replace();
	</script>
</body>
</html><!-- END:   inst/pkgdown/templates/layout.html-->

